===== api-500-spike.txt =====

# API 500 Spike

Symptoms:
- Sudden increase in HTTP 500 responses
- User-facing errors and failed requests
- Error rate alarms triggered

Likely Causes:
- Regression in recent deployment
- Downstream dependency failures
- Unhandled exceptions or misconfiguration

Immediate Mitigation:
- Roll back the latest deployment
- Check downstream health and circuit breakers
- Increase logging around failing endpoints

Escalation:
- Escalate to service owner if rollback fails
- Engage dependency owners for persistent failures

Verification:
- 500 error rate returns to baseline
- Successful request rate recovers
- No new errors in logs after rollback


===== auth-token-failure.txt =====

# Auth Token Failure

Symptoms:
- Authentication failures or 401 spikes
- Users unable to log in or access APIs
- Token validation errors in logs

Likely Causes:
- Expired signing keys or rotation issues
- Clock skew between services
- Identity provider degradation

Immediate Mitigation:
- Verify key rotation status and token issuer
- Check time sync across services
- Fail over to secondary identity provider if available

Escalation:
- Escalate to IAM team for issuer issues
- Engage security team if tokens are compromised

Verification:
- 401/403 rates return to baseline
- Authentication flow succeeds for test users
- Token validation errors disappear


===== custom-incidents.txt =====

# Custom Incident Runbook Entries

# Append new entries generated by draft_runbook_entry below this line.


===== database-timeouts.txt =====

# Database Timeouts

Symptoms:
- Application requests timing out on database calls
- Increased query latency and connection pool saturation
- Error messages mentioning timeout or connection limits

Likely Causes:
- Slow or blocked queries
- Connection pool exhausted
- Storage or network latency to the database

Immediate Mitigation:
- Scale connection pool or application replicas cautiously
- Identify top slow queries and terminate long-running sessions
- Fail over to a healthy replica if available

Escalation:
- Escalate to database on-call if timeouts exceed 10 minutes
- Engage storage/network team if I/O latency is elevated

Verification:
- Query latency returns to baseline
- Error rate drops to normal levels
- Connection pool utilization stabilizes


===== deployment-failure.txt =====

# Deployment Failure

Symptoms:
- Deployment pipeline fails or stalls
- New pods fail readiness checks
- Rollout stuck or rolled back automatically

Likely Causes:
- Bad configuration or missing env vars
- Image build or registry issues
- Migration or schema changes failed

Immediate Mitigation:
- Roll back to last successful release
- Validate configs and secrets
- Pause rollout and investigate pipeline logs

Escalation:
- Escalate to release engineering
- Engage DB team if migrations are involved

Verification:
- Deployment completes successfully
- Health checks pass for new version
- No rollback events observed


===== flaky-tests-ci.txt =====

# Flaky Tests in CI

Symptoms:
- CI failures intermittent with no code changes
- Tests pass on retry
- Longer build times and pipeline noise

Likely Causes:
- Non-deterministic test setup
- Shared test resources or timeouts
- External dependency instability

Immediate Mitigation:
- Quarantine flaky tests to reduce noise
- Increase timeouts for unstable suites
- Stabilize test data and mocks

Escalation:
- Escalate to QA/DevEx for flaky test backlog
- Engage owners of flaky suites

Verification:
- CI pass rate improves without retries
- Flaky test list decreases over time
- Pipeline duration stabilizes


===== high-cpu.txt =====

# High CPU

Symptoms:
- CPU utilization sustained above 85%
- Increased response times
- Autoscaling triggered frequently

Likely Causes:
- Expensive queries or hot code path
- Traffic surge
- Runaway background jobs

Immediate Mitigation:
- Scale out service instances
- Throttle non-critical workloads
- Identify top CPU consumers and isolate

Escalation:
- Escalate if CPU remains high after scaling
- Engage application team for profiling

Verification:
- CPU drops to normal range
- Latency stabilizes
- Autoscaling events reduce


===== k8s-crashloop.txt =====

# Kubernetes CrashLoopBackOff

Symptoms:
- Pods repeatedly restarting
- CrashLoopBackOff events in cluster
- Service availability degraded

Likely Causes:
- Bad configuration or missing secrets
- Application startup failure
- Resource limits too low

Immediate Mitigation:
- Check pod logs for startup errors
- Roll back to last known good image
- Increase memory or CPU limits if constrained

Escalation:
- Escalate to platform team for cluster-wide issues
- Engage service owner for code-level fixes

Verification:
- Pods remain healthy for multiple restart cycles
- Service endpoints pass health checks
- Error rate decreases


===== latency-spike.txt =====

# Latency Spike

Symptoms:
- P95/P99 latency exceeds SLO
- Slow responses without full outages
- Increased queue or thread pool wait times

Likely Causes:
- Resource contention (CPU, memory, I/O)
- Dependency slowness
- Traffic surge or thundering herd

Immediate Mitigation:
- Scale out service instances
- Enable caching or increase cache TTL
- Shed load for non-critical endpoints

Escalation:
- Escalate if latency persists beyond 15 minutes
- Coordinate with dependency teams if they are degraded

Verification:
- Latency metrics return to SLO
- Queue depths normalize
- Error rate remains stable


===== memory-leak.txt =====

# Memory Leak

Symptoms:
- Gradual increase in memory usage over time
- OOM kills or container restarts
- GC pressure and latency increases

Likely Causes:
- Unbounded caches or collections
- Resource handles not released
- Leaky third-party library

Immediate Mitigation:
- Restart affected instances to stabilize
- Reduce load or scale out to buy time
- Enable memory profiling if available

Escalation:
- Escalate to engineering for code fix
- Engage SRE for runtime diagnostics

Verification:
- Memory usage stabilizes after mitigation
- No new OOM events
- Latency improves


===== redis-connection-issues.txt =====

# Redis Connection Issues

Symptoms:
- Connection refused or timeout errors to Redis
- Cache miss rate spikes
- Elevated latency on cache-dependent endpoints

Likely Causes:
- Redis node down or networking issues
- Connection pool exhaustion
- Authentication or TLS misconfiguration

Immediate Mitigation:
- Fail over to replica or managed cache
- Increase connection pool size if safe
- Temporarily bypass cache for critical requests

Escalation:
- Escalate to cache platform team if node is down
- Engage network team for connectivity issues

Verification:
- Cache connections succeed
- Cache hit rate stabilizes
- Latency improves for cache-backed calls


